{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Donald Trump Speech With A Recurrent Neural Network\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that follows all the work done in _\"Generating A Shakespeare Text With A Recurrent Neural Network\"_, so I won't go into details. The data used in this notebook can be found in https://www.kaggle.com/arnavsharmaas/all-donald-trump-transcripts. Please download it and save it into the folder where this notebook is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('trump_3.6.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "text = text #the text has more than 3 million characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [char for char in set(text)] #print if you want to have a look at the set of characters in the text\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocabulary)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = np.array(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 100 #we have to define the length of the sequences\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) #batch must be of size seq_length+1 so we can have displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) #https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim,rnn_units,batch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #First layer\n",
    "    model.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        batch_input_shape = [batch_size,None])) \n",
    "    \n",
    "    #Second layer\n",
    "    \n",
    "    model.add(LSTM(rnn_units, #number of neurons in the layer\n",
    "                   return_sequences = True, #we specify that we want to predict the character following each of the input characters,\n",
    "                                            #not only of the last one\n",
    "                   stateful = True, # f True, the last state for each sample at index i in a batch will be used as initial\n",
    "                                    #state for the sample of index i in the following batch.\n",
    "                   recurrent_initializer = 'glorot_uniform' #indicates how internal weight matrices must be initialized\n",
    "                    )) \n",
    "    \n",
    "    #Third layer\n",
    "    \n",
    "    model.add(Dense(vocab_size)) \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 64 #arbitrary\n",
    "rnn_units = 1024 \n",
    "\n",
    "model = build_model(\n",
    "vocab_size = vocab_size,\n",
    "embedding_dim = embedding_dim,\n",
    "rnn_units = rnn_units,\n",
    "batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def loss(labels,logits): #logits are the \"predicted values\" (likelihoods in this case)\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "562/562 [==============================] - 2689s 5s/step - loss: 2.7812\n",
      "Epoch 2/50\n",
      "562/562 [==============================] - 2847s 5s/step - loss: 1.6739\n",
      "Epoch 3/50\n",
      "562/562 [==============================] - 2847s 5s/step - loss: 1.3518\n",
      "Epoch 4/50\n",
      "562/562 [==============================] - 2995s 5s/step - loss: 1.2181\n",
      "Epoch 5/50\n",
      "562/562 [==============================] - 2904s 5s/step - loss: 1.1467\n",
      "Epoch 6/50\n",
      "562/562 [==============================] - 2852s 5s/step - loss: 1.0992\n",
      "Epoch 7/50\n",
      "562/562 [==============================] - 2816s 5s/step - loss: 1.0621\n",
      "Epoch 8/50\n",
      "562/562 [==============================] - 2944s 5s/step - loss: 1.0324\n",
      "Epoch 9/50\n",
      "562/562 [==============================] - 2665s 5s/step - loss: 1.0082\n",
      "Epoch 10/50\n",
      "562/562 [==============================] - 2765s 5s/step - loss: 0.9853\n",
      "Epoch 11/50\n",
      "562/562 [==============================] - 2935s 5s/step - loss: 0.9663\n",
      "Epoch 12/50\n",
      "562/562 [==============================] - 2929s 5s/step - loss: 0.9479\n",
      "Epoch 13/50\n",
      "562/562 [==============================] - 2916s 5s/step - loss: 0.9324\n",
      "Epoch 14/50\n",
      "562/562 [==============================] - 2928s 5s/step - loss: 0.9172\n",
      "Epoch 15/50\n",
      "562/562 [==============================] - 2896s 5s/step - loss: 0.9019\n",
      "Epoch 16/50\n",
      "562/562 [==============================] - 3084s 5s/step - loss: 0.8888\n",
      "Epoch 17/50\n",
      "562/562 [==============================] - 3033s 5s/step - loss: 0.8755\n",
      "Epoch 18/50\n",
      "562/562 [==============================] - 2890s 5s/step - loss: 0.8638\n",
      "Epoch 19/50\n",
      "562/562 [==============================] - 2799s 5s/step - loss: 0.8543\n",
      "Epoch 20/50\n",
      "562/562 [==============================] - 2876s 5s/step - loss: 0.8431\n",
      "Epoch 21/50\n",
      "562/562 [==============================] - 2870s 5s/step - loss: 0.8343\n",
      "Epoch 22/50\n",
      "562/562 [==============================] - 2890s 5s/step - loss: 0.8238\n",
      "Epoch 23/50\n",
      "562/562 [==============================] - 2865s 5s/step - loss: 0.8137\n",
      "Epoch 24/50\n",
      "562/562 [==============================] - 2189s 4s/step - loss: 0.8059\n",
      "Epoch 25/50\n",
      "562/562 [==============================] - 1158s 2s/step - loss: 0.7961\n",
      "Epoch 26/50\n",
      "562/562 [==============================] - 1172s 2s/step - loss: 0.7898\n",
      "Epoch 27/50\n",
      "562/562 [==============================] - 1164s 2s/step - loss: 0.7828\n",
      "Epoch 28/50\n",
      "562/562 [==============================] - 1185s 2s/step - loss: 0.7755\n",
      "Epoch 29/50\n",
      "562/562 [==============================] - 1164s 2s/step - loss: 0.7684\n",
      "Epoch 30/50\n",
      "562/562 [==============================] - 1168s 2s/step - loss: 0.7616\n",
      "Epoch 31/50\n",
      "562/562 [==============================] - 1195s 2s/step - loss: 0.7548\n",
      "Epoch 32/50\n",
      "562/562 [==============================] - 1181s 2s/step - loss: 0.7482\n",
      "Epoch 33/50\n",
      "562/562 [==============================] - 1184s 2s/step - loss: 0.7430\n",
      "Epoch 34/50\n",
      "562/562 [==============================] - 1157s 2s/step - loss: 0.7384\n",
      "Epoch 35/50\n",
      "562/562 [==============================] - 1187s 2s/step - loss: 0.7321\n",
      "Epoch 36/50\n",
      "562/562 [==============================] - 1183s 2s/step - loss: 0.7290\n",
      "Epoch 37/50\n",
      "562/562 [==============================] - 1182s 2s/step - loss: 0.7229\n",
      "Epoch 38/50\n",
      "562/562 [==============================] - 1195s 2s/step - loss: 0.7183\n",
      "Epoch 39/50\n",
      "562/562 [==============================] - 2694s 5s/step - loss: 0.7130\n",
      "Epoch 40/50\n",
      "562/562 [==============================] - 2853s 5s/step - loss: 0.7093\n",
      "Epoch 41/50\n",
      "562/562 [==============================] - 2759s 5s/step - loss: 0.7069\n",
      "Epoch 42/50\n",
      "562/562 [==============================] - 2815s 5s/step - loss: 0.7036\n",
      "Epoch 43/50\n",
      "562/562 [==============================] - 2918s 5s/step - loss: 0.6984\n",
      "Epoch 44/50\n",
      "562/562 [==============================] - 3220s 6s/step - loss: 0.6962\n",
      "Epoch 45/50\n",
      "562/562 [==============================] - 2953s 5s/step - loss: 0.6929\n",
      "Epoch 46/50\n",
      "562/562 [==============================] - 2090s 4s/step - loss: 0.6913\n",
      "Epoch 47/50\n",
      "562/562 [==============================] - 2005s 4s/step - loss: 0.6880\n",
      "Epoch 48/50\n",
      "562/562 [==============================] - 2661s 5s/step - loss: 0.6854\n",
      "Epoch 49/50\n",
      "562/562 [==============================] - 2916s 5s/step - loss: 0.6822\n",
      "Epoch 50/50\n",
      "562/562 [==============================] - 4467s 8s/step - loss: 0.6801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "import os \n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_prefix,\n",
    "        save_weights_only=True)\n",
    "\n",
    "\n",
    "epochs= 50\n",
    "\n",
    "history = model.fit(dataset,epochs=epochs,callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "vocab_size = vocab_size,\n",
    "embedding_dim = embedding_dim,\n",
    "rnn_units = rnn_units,\n",
    "batch_size = 1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,start_string,char2idx, idx2char,num_generate = 500):\n",
    "    \n",
    "    #we convert the initial input to numerical representation\n",
    "    \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    \n",
    "    input_eval = tf.expand_dims(input_eval,0) #expanded to match batch format shape\n",
    "    \n",
    "    text_generated = [] #generated text will be stored here\n",
    "    \n",
    "    temperature = 0.4 # if temperature =0, model is very conservative. If temperature = 1, model is very creative (but riskier)\n",
    "    \n",
    "    model.reset_states()#Resets all of the metric state variables.\n",
    "    \n",
    "    for i in range(num_generate): #loop to generate characters\n",
    "        predictions = model(input_eval) #generate prediction\n",
    "        predictions = tf.squeeze(predictions,0) #remove batch format\n",
    "        predictions = predictions / temperature #added to affect probability of next character\n",
    "        predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy() #next character is selected following\n",
    "                                                                                      #categorical distribution\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id],0) #predicted character passed as next input\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id]) #character is added to text in character format\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome.\r\n",
      "Mr. President, I cannot describe it. This is the greatest economy in history and now we have to come together and we have the greatest economy in history, and we will be there and they show with all of the things that we did that we have to look at it. They don’t want to talk about it. They don’t like him. So I said, “We have to do this.” And they said, “Well, what do you think?” I said, “You know, the way you want to see a lot of money on the moon and the United States will be the first nati\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,char2idx=char2idx, idx2char = idx2char, start_string=u\"Welcome\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "\n",
    "https://medium.com/towards-artificial-intelligence/create-your-first-text-generator-with-lstm-in-few-minutes-3b59ee139ca0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
