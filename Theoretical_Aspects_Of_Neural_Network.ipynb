{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Neural Network\n",
    "\n",
    "In this notebook we will briefly go through some of the theoretical aspects of the neural network training.\n",
    "\n",
    "Remember that the neural network learning is just the process through which we adjust the parameters and bias of the network for a given group of data. In order to make these adjustments, we need to take into account two things: the _loss function_ and the _optimizator_:.\n",
    "\n",
    "The _loss function_ is basically a way of measuring the error between the neural network predictions and the true labels. The value given by the loss function is then used by the _optimizator_, which is in charge of adjusting the neural network parameters in order to minimize the error.\n",
    "\n",
    "In essence, training a neural network requires three steps:\n",
    "\n",
    "- 1. Forward propagation : data is introduced in the neural network, passes through all the layers and ends up giving a prediction.\n",
    "- 2. Error estimation: we use a loss function to estimate the error and see how good our model is.\n",
    "      \n",
    "- 3. Backward propagation: neural network parameters are adjusted with an optimizator in order to minimize the error given by the loss function in a backward direction (i.e. the first neurons to be adjusted are the ones that are closer to the outcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "We will now cover one of the most popular optimizators in Deep Learning, the _Gradient Descent_, and some of its variations (_Batch Gradient Descent_ ,_Mini Batch Gradient Descent_ and _Stochastic Gradient Descent_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradient Descent\n",
    "\n",
    "The basic idea of this optimization is the adjustment of the parameters iteratively so as to minimize the loss functions. \n",
    "\n",
    "The gradient descent uses the first derivative (hence _gradient_) of the loss function, which is chained with the derivatives of each network layer by applying the chain rule. The application of the chain rule is essentially what leads to the backpropagation.  Since the gradient at a point ($w$ in the figure below) goes in the direction for which the function increases, we must choose the negative of it. By doing this, if we substract the negative gradient multiplied by a factor ($\\alpha$ in the figure) to the initial point  we make the function follow the direction of steepest descent, which ends up leading to a minimum (for more information, specially the equations, please have a look at https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg\" width=\"400\" height=\"400\" />\n",
    "\n",
    "\n",
    "Bear in mind that the figure is a simplified case, as we have not performed the chain rule in the gradient.\n",
    "\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "In order to adjust the parameters, we use the average gradient of the whole dataset in each iteration/epoch.\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "In this case we use a single sample of the dataset per iteration instead of all of them to adjust a parameter. In general, this method performs better, but it is not suitable for all the optimizations techniques \n",
    "\n",
    "#### Mini Batch Gradient Descent\n",
    "\n",
    "This technique lies in the middle of the previous two, as it uses the average gradient of a subset from the dataset instead of a single sample. This method usually performs better than the other two. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to choose the batch size in Keras, we can use the following line:\n",
    "\n",
    "\n",
    "` model.fit(x_train,y_train,epochs=5,batch_size=100)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "We have already seen in previous notebooks one loss function, which is the _categorical_crossentropy_. This loss function was implemented because we were required a categorical outcome. In this case, there must be the same number of neurons in the last layer as outcomes (this is why we used the _softmax_ activation function) However, if we are dealing with a binary classification, we usually use _binary_crossentropy_ as the loss function and _sigmoid_ as the activation function.\n",
    "\n",
    "As you can see, depending on the type of problem we are dealing with, one loss function might be more suitable than other. For this reason, we will introduce the different loss functions in the notebooks where they are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers \n",
    "\n",
    "Keras offers the possibility of using plenty of optimizers. In addition to that, it also allows to change their hyperparameters. As an example, have a look at the code below: \n",
    "\n",
    "```python\n",
    "\n",
    "from tensorflow.keras.optimizer import RMSprop #this is a type of optimizer\n",
    "\n",
    "my_optimizer = tf.keras.optimizers.RMSprop(0.001) #changing learning rate to 0.001\n",
    "model.compile(optimizer=my_optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "          \n",
    "```\n",
    "\n",
    "When calling the optimizer we added a parameter, the _learning rate_, which we will explain in following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "_Python Deep Learning_ , by Jordi Torres (https://www.marcombo.com/python-deep-learning-9788426728289/)\n",
    "\n",
    "Gradient Descent image extracted from _PyTorch Lecture 3: Gradient Descent_ (https://www.youtube.com/watch?v=b4Vyma9wPHo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
